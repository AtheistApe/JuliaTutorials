{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Data\n",
    "\n",
    "JuliaDB can distributed datasets across multiple processes and can work with data larger than available memory (RAM).\n",
    "\n",
    "First let's start by adding some worker Julia processes.  If you do not specify a number, `addprocs` will add as many processes as there are CPU cores available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we take advantage of these worker processes, let's load `JuliaDB`. We can also use the command `IndexedTables.set_show_compact!(false)` to make sure that we default to seeing full data tables rather than summaries of their fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuliaDB\n",
    "\n",
    "IndexedTables.set_show_compact!(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When multiple processes are available, `loadtable` will now create distributed tables.\n",
    "\n",
    "Note the line above the column names printed below: \n",
    "```\n",
    "Distributed Table with 56023 rows in 2 chunks\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = loadtable(\"stocksample\", filenamecol = :Ticker, indexcols = [:Ticker, :Date])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notable difference 1: No `getindex`\n",
    "\n",
    "Now that we're working with a distributed table, indexing will no longer work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notable difference 2: Not iterable\n",
    "\n",
    "Similarly, we're not able to iterate over the contents of a distributed table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dt\n",
    "    println(row)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring Distributed Table Into Master Process\n",
    "\n",
    "While not necessary for most operations, you may occasionally want to bring a dataset into the master process. This will allow you to, for example, iterate over the distributed table's rows.  This is accomplished with the `collect` function.\n",
    "\n",
    "Note that after `collect`ing, the table's header says `Table` instead of `Distributed Table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = collect(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries still work on a distributed table!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions that return a single value still return a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce(+, dt; select = :Close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions that returned a Table now return a Distributed Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupreduce(+, dt, :Ticker; select = :Close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions that returned an `Array` now return a `DArray` (distributed array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select(dt, :Close)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just like for tables, `collect` will change a `DArray` to an `Array`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect(select(dt, :Close))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Core Functionality\n",
    "\n",
    "Reference: http://juliadb.org/latest/manual/out-of-core.html\n",
    "\n",
    "JuliaDB can be used to load/query datasets that are too big to fit in memory (RAM).\n",
    "\n",
    "#### How this works:\n",
    "\n",
    "*Data is loaded into a distributed dataset in *chunks* that fit in memory.*\n",
    "\n",
    "The `loadtable` and `loadndsparse` functions take an `output` keyword argument which can be set to a directory where the loaded data is written to in an efficient binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadtable(\"stocksample\", output = \"bin\", filenamecol=:Ticker, indexcols=[:Ticker, :Date])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything below is a WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data is loaded into a distributed dataset containing *chunks* that are small enough to fit in memory.\n",
    "1. Data is processed `p` chunks at a time â€“ where `p` is the number of worker processes. This means `p * size of chunks` should fit in memory!\n",
    "1. Output data (from `reduce`, etc.) is accumulated in-memory and must be small enough to fit in the available memory.\n",
    "\n",
    "Several queries are designed to work on such datasets:\n",
    "\n",
    "1. `reduce`\n",
    "1. `groupreduce`\n",
    "\n",
    "#### Loading Data Out-of-Core\n",
    "\n",
    "Out-of-core processing is achieved using the `output` and `chunks` keyword arguments. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadtable(\"stocksample\", output = \"bin\", chunks = 8,\n",
    "    filenamecol=:Ticker, indexcols=[:Ticker, :Date])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StatPlots\n",
    "\n",
    "The StatPlots package integrates with Plots to plot a variety of tabular data structures, including those in JuliaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using StatPlots\n",
    "gr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we could generate a plot simply by calling the `plot` function. When working with tables (as well as DataFrames, DataStreams, etc.), we need to precede our call to `plot` with the `@df` macro and the name of the table. This @df command allows the plot call to manipulate a table's columns as symbols as if they were `Array`s.\n",
    "\n",
    "For example, the syntax\n",
    "```julia\n",
    "@df tablename plot(:x, :y)\n",
    "```\n",
    "allows us to plot the columns `x` and `y` of the table `tablename`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = table(@NT(x = [1, 2, 3], y = [1, 4, 9]))\n",
    "@df tablename plot(:x, :y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot a distributed table with columns `x` and `y`, we would use the syntax\n",
    "\n",
    "```julia\n",
    "@df collect(tablename) plot(:x, :y)\n",
    "```\n",
    "\n",
    "To plot our stock data currently stored in the distributed table, `dt`, by stock (`Ticker` field), we use the `group` keyword argument to `plot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@df collect(dt) plot(:Date, :Close; group=:Ticker, legend=:topleft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition plots\n",
    "\n",
    "If you are trying to distribute your data in the first place, it's probably big. In this case, you may want to use `partitionplot` which\n",
    "\n",
    "- recognizes that when data is huge it doesn't make sense to plot every point.\n",
    "- incrementally builds summaries of the data.  \n",
    "- relies on fixed-size memory data structures that can handle infinite data streams.\n",
    "\n",
    "**We'll see more on this in the next notebook, but for now let's try it out!**\n",
    "\n",
    "The syntax is\n",
    "\n",
    "```julia\n",
    "partitionplot(tablename, :x, :y)\n",
    "```\n",
    "\n",
    "for a table, `tablename`, with columns `x` and `y`.\n",
    "\n",
    "Note that here we need neither the `@df` macro nor a call to `collect` (for distributed tables).\n",
    "\n",
    "For example, the following command plots the data from `dt` by stock (`Ticker` field) using the `by` keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "partitionplot(dt, :Date, :Close; by=:Ticker, legend=:topleft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
